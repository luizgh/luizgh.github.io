---
layout: post
date: 2025-05-09 14:05:00-0400
inline: true
related_posts: false
---

SIGGRAPH 2025: Our paper titled **Model See Model Do: Speech-Driven Facial Animation with Style Control** was accepted for SIGGRAPH 2025.   
<br />
   
We present an example-based diffusion model that generates stylistic 3D facial animations. The generated animations are lip-synced to a provided audio track, and adhere to the style delivery of the example animation. Our quantitative experiments and user-studies show improved style adherence compared to past methods that used contrastive methods for learning style. More details in the [project page](https://ubisoft-laforge.github.io/character/msmd/).